const fs = require("fs");
const path = require("path");

const rootDir = process.cwd();

const files = {
  // 1. [Backend] ìŠ¤í‚¤ë§ˆì— use_deep_think í•„ë“œ ì¶”ê°€
  "backend/app/schemas/chat.py": `from pydantic import BaseModel
from typing import Optional, List

class ChatRequest(BaseModel):
    message: str
    kb_id: str
    use_web_search: bool = False
    use_deep_think: bool = False # âœ… ì¶”ê°€ë¨
    active_mcp_ids: List[str] = []
`,

  // 2. [Backend] ì—”ë“œí¬ì¸íŠ¸ì—ì„œ íŒŒë¼ë¯¸í„° ì „ë‹¬ ìˆ˜ì •
  "backend/app/api/endpoints/chat.py": `from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse
from app.schemas.chat import ChatRequest
from app.services.rag_service import RAGService
from app.api.deps import get_current_user
from app.models.user import User

router = APIRouter()

def get_rag_service():
    return RAGService()

@router.post("/stream")
async def chat_stream(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    service: RAGService = Depends(get_rag_service)
):
    return StreamingResponse(
        service.generate_response(
            message=request.message,
            kb_id=request.kb_id,
            user_id=current_user.id,
            use_web_search=request.use_web_search,
            use_deep_think=request.use_deep_think, # âœ… ì „ë‹¬
            active_mcp_ids=request.active_mcp_ids
        ),
        media_type="text/event-stream"
    )
`,

  // 3. [Backend] ì„œë¹„ìŠ¤ ë¡œì§ì— ì¡°ê±´ë¬¸ ì ìš©
  "backend/app/services/rag_service.py": `import json
import asyncio
import os
from typing import AsyncGenerator, List, Optional
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools import DuckDuckGoSearchRun
from app.core.config import settings
from app.services.vector_store import VectorStoreService
from app.services.xlam_service import XLAMService

class RAGService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService()
        self.xlam_service = XLAMService()
        
        self.llm = ChatOllama(model=settings.LLM_MODEL, temperature=0)
        self.web_search_tool = DuckDuckGoSearchRun()

    async def generate_response(
        self, 
        message: str, 
        kb_id: str, 
        user_id: int, 
        use_web_search: bool = False,
        use_deep_think: bool = False, # âœ… íŒŒë¼ë¯¸í„° ì¶”ê°€
        active_mcp_ids: Optional[List[str]] = None
    ) -> AsyncGenerator[str, None]:
        
        try:
            # [Router] ì§ˆë¬¸ ì˜ë„ ë¶„ì„ (Deep Thinkingì´ ì¼œì ¸ìˆê±°ë‚˜, ëª¨í˜¸í•  ë•Œ ìˆ˜í–‰)
            route = "rag"
            
            if use_deep_think: # âœ… ë”¥ ì”½í‚¹ í™œì„±í™” ì‹œì—ë§Œ ë¶„ì„ ê³¼ì • ë…¸ì¶œ
                yield json.dumps({"type": "thinking", "thinking": "ğŸ¤” ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."}) + "\\n"
                
                router_prompt = ChatPromptTemplate.from_template("""
                Analyze the user's question and choose the best processing mode.
                Question: {question}
                Options: 'process' (logistics/business execution), 'search' (real-time info), 'rag' (documents), 'chat' (general).
                Answer (process/search/rag/chat):
                """)
                router_chain = router_prompt | self.llm | StrOutputParser()
                route_result = await router_chain.ainvoke({"question": message})
                route = route_result.strip().lower()
                
                yield json.dumps({"type": "thinking", "thinking": f"ğŸ§­ ë¶„ì„ ê²°ê³¼: '{route}' ëª¨ë“œë¡œ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."}) + "\\n"
            
            else:
                # ë”¥ ì”½í‚¹ êº¼ì ¸ìˆìœ¼ë©´ ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
                if use_web_search: route = "search"
                elif any(k in message for k in ["ë°°ì°¨", "ì£¼ë¬¸", "ë£¨íŠ¸", "ì§€ì‹œ"]): route = "process"
                else: route = "rag"

            # --- [MODE 1] xLAM Process Execution ---
            if "process" in route:
                yield json.dumps({"type": "thinking", "thinking": "ğŸš€ xLAM ììœ¨ ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤."}) + "\\n"
                async for chunk in self.xlam_service.run_pipeline(message, kb_id, user_id):
                    yield chunk
                return

            context_text = ""
            
            # --- [MODE 2] Web Search ---
            if "search" in route:
                if use_deep_think: yield json.dumps({"type": "thinking", "thinking": "ğŸŒ ì›¹ ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤..."}) + "\\n"
                try:
                    res = self.web_search_tool.invoke(message)
                    context_text = f"[Web Search Result]\\n{res}"
                except:
                    context_text = "ê²€ìƒ‰ ì‹¤íŒ¨"
                    
            # --- [MODE 3] RAG (Document Search) ---
            else:
                if use_deep_think: yield json.dumps({"type": "thinking", "thinking": f"ğŸ” ì§€ì‹ ë² ì´ìŠ¤({kb_id})ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ íƒìƒ‰ ì¤‘..."}) + "\\n"
                retriever = self.vector_service.get_retriever(kb_id, user_id)
                docs = await retriever.ainvoke(message)
                if docs:
                    context_text = "\\n\\n".join([doc.page_content for doc in docs])
                    if use_deep_think: yield json.dumps({"type": "thinking", "thinking": f"âœ… ë¬¸ì„œ {len(docs)}ê°œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤."}) + "\\n"
                else:
                    context_text = ""
                    if use_deep_think: yield json.dumps({"type": "thinking", "thinking": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}) + "\\n"

            # ë‹µë³€ ìƒì„±
            prompt = ChatPromptTemplate.from_template("""
            [ë¬¸ë§¥]
            {context}
            [ì§ˆë¬¸]
            {question}
            ë‹µë³€í•´ì£¼ì„¸ìš”:
            """)
            chain = prompt | self.llm
            full_response = ""
            async for chunk in chain.astream({"context": context_text, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += content
                yield json.dumps({"type": "content", "content": content}) + "\\n"

            # [Self-Correction] ìê¸° ê²€ì¦ (Deep Thinking ì¼œì ¸ìˆì„ ë•Œë§Œ)
            if use_deep_think and len(full_response) > 50:
                yield json.dumps({"type": "thinking", "thinking": "ğŸ›¡ï¸ ë‹µë³€ì˜ ì •í™•ì„±ì„ ìì²´ ê²€ì¦(Self-Reflection) ì¤‘..."}) + "\\n"
                reflection_prompt = ChatPromptTemplate.from_template("""
                Question: {question}
                Answer: {answer}
                Rate the answer's accuracy (0-100). Output only the number.
                """)
                score = await (reflection_prompt | self.llm | StrOutputParser()).ainvoke({"question": message, "answer": full_response})
                try:
                    if int(''.join(filter(str.isdigit, score))) > 80:
                         yield json.dumps({"type": "thinking", "thinking": "âœ¨ ê²€ì¦ ì™„ë£Œ: ì‹ ë¢°ë„ ë†’ìŒ"}) + "\\n"
                except: pass

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            yield json.dumps({"type": "content", "content": f"Error: {str(e)}"}) + "\\n"
`,

  // 4. [Frontend] API Clientì—ì„œ useDeepThink ì „ì†¡
  "frontend/src/api/client.js": `import axios from 'axios';

const API_BASE_URL = 'http://localhost:8000/api/v1';

const getAuthHeader = () => {
  const token = localStorage.getItem('rag_token');
  return token ? { 'Authorization': \`Bearer \${token}\` } : {};
};

// use_deep_think íŒŒë¼ë¯¸í„° ì¶”ê°€
export const streamChat = async ({ query, model, kb_id, web_search, use_deep_think, active_mcp_ids }, onChunk, onComplete) => {
  try {
    const response = await fetch(\`\${API_BASE_URL}/chat/stream\`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        ...getAuthHeader()
      },
      body: JSON.stringify({
        message: query,
        kb_id: kb_id || "default_kb",
        use_web_search: web_search || false,
        use_deep_think: use_deep_think || false, // âœ… ì „ì†¡
        active_mcp_ids: active_mcp_ids || []
      })
    });

    if (!response.ok) {
      if (response.status === 401) throw new Error("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.");
      throw new Error(\`Network response was not ok: \${response.status}\`);
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = "";

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\\n');
      buffer = lines.pop(); 

      for (const line of lines) {
        if (!line.trim()) continue;
        try {
          const jsonStr = line.startsWith('"') && line.endsWith('"') ? JSON.parse(line) : line;
          const data = typeof jsonStr === 'object' ? jsonStr : JSON.parse(jsonStr);
          onChunk(data);
        } catch (e) { console.error("Parse Error:", e); }
      }
    }
    if (onComplete) onComplete();

  } catch (error) {
    console.error("Stream Error:", error);
    onChunk({ type: 'content', content: \`\\n[Error] \${error.message}\` });
    if (onComplete) onComplete();
  }
};

export const uploadFileToBackend = async (file, kbId, chunkSize, chunkOverlap) => {
  const formData = new FormData();
  formData.append('file', file);
  formData.append('kb_id', kbId || "default_kb");
  if (chunkSize) formData.append('chunk_size', chunkSize);
  if (chunkOverlap) formData.append('chunk_overlap', chunkOverlap);

  const response = await fetch(\`\${API_BASE_URL}/knowledge/upload\`, {
    method: 'POST',
    headers: { ...getAuthHeader() },
    body: formData
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(errorText || "Upload failed");
  }
  return await response.json();
};
`,
};

function fixDeepThink() {
  console.log("ğŸš€ Deep Thinking ì—°ê²° ë³µêµ¬ ì¤‘...");
  for (const [relPath, content] of Object.entries(files)) {
    const fullPath = path.join(rootDir, relPath);
    const dir = path.dirname(fullPath);
    if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });
    fs.writeFileSync(fullPath, content.trim(), "utf8");
    console.log(`âœ… ìˆ˜ì •ë¨: ${relPath}`);
  }
  console.log(
    "\\nğŸ‰ ì—°ê²° ì™„ë£Œ! ë°±ì—”ë“œë¥¼ ì¬ì‹œì‘í•˜ë©´ 'Deep Thinking' ë²„íŠ¼ì´ ì‘ë™í•©ë‹ˆë‹¤."
  );
}

fixDeepThink();
