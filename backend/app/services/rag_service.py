import json
import asyncio
import os
from typing import AsyncGenerator, List, Optional
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from app.core.config import settings
from app.services.vector_store import VectorStoreService

class RAGService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService()
        
        self.llm = ChatOllama(
            model=settings.LLM_MODEL,
            temperature=0.1,
        )

    async def generate_response(
        self, 
        message: str, 
        kb_id: str, 
        user_id: int, 
        use_web_search: bool = False,
        active_mcp_ids: Optional[List[str]] = None
    ) -> AsyncGenerator[str, None]:
        
        try:
            # 1. ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
            yield json.dumps({"type": "thinking", "thinking": f"ğŸ” ì§€ì‹ ë² ì´ìŠ¤({kb_id}) ê²€ìƒ‰ ì¤‘..."}) + "\n"
            
            # 2. ë²¡í„° ê²€ìƒ‰
            retriever = self.vector_service.get_retriever(kb_id, user_id)
            docs = await retriever.ainvoke(message)
            
            context_text = ""
            if docs:
                # [ë””ë²„ê¹…] ì‹¤ì œ LLMì— ë“¤ì–´ê°€ëŠ” í…ìŠ¤íŠ¸ê°€ ë¬´ì—‡ì¸ì§€ ì„œë²„ ë¡œê·¸ì— ì¶œë ¥
                print(f"--- [RAG Context Retrieved] ---")
                for i, doc in enumerate(docs):
                    # ë²¡í„° ë°ì´í„°ê°€ í…ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì •ì œ
                    clean_content = doc.page_content.replace("{", "").replace("}", "") # JSON ê´„í˜¸ ê°™ì€ê±° ì œê±° ì‹œë„
                    # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ë¡œê·¸ í™•ì¸
                    print(f"Doc {i+1}: {doc.page_content[:100]}...") 
                    
                print(f"-------------------------------")

                context_text = "\n\n".join([doc.page_content for doc in docs])
                sources = list(set([doc.metadata.get("source", "Unknown") for doc in docs]))
                yield json.dumps({"type": "thinking", "thinking": f"âœ… ë¬¸ì„œ {len(docs)}ê°œ ì°¸ì¡°: {', '.join(sources)}"}) + "\n"
            else:
                yield json.dumps({"type": "thinking", "thinking": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}) + "\n"
            
            # 3. í”„ë¡¬í”„íŠ¸ (ê°•ë ¥í•œ ì§€ì‹œì‚¬í•­ ì¶”ê°€)
            prompt = ChatPromptTemplate.from_template("""
            ë‹¹ì‹ ì€ RAG(Retrieval-Augmented Generation) AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ì•„ë˜ [ë¬¸ë§¥]ì— ì œê³µëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ì„¸ìš”.
            
            ì¤‘ìš”:
            1. [ë¬¸ë§¥]ì— 'Vector', 'Dense', 'Sparse' ê°™ì€ ë°ì´í„° êµ¬ì¡°ê°€ ë³´ì´ë©´ ë¬´ì‹œí•˜ê³ , ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì„¸ìš”.
            2. ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
            3. ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ í•˜ì„¸ìš”.

            [ë¬¸ë§¥]
            {context}
            
            [ì§ˆë¬¸]
            {question}
            """)
            
            chain = prompt | self.llm
            
            # 4. ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
            async for chunk in chain.astream({"context": context_text, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                yield json.dumps({"type": "content", "content": content}) + "\n"

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            yield json.dumps({"type": "content", "content": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}"}) + "\n"