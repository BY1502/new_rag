import json
import asyncio
import os
from typing import AsyncGenerator, List, Optional
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools import DuckDuckGoSearchRun
from app.core.config import settings
from app.services.vector_store import VectorStoreService
from app.services.xlam_service import XLAMService # âœ… ì¶”ê°€

class RAGService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService()
        self.xlam_service = XLAMService() # âœ… xLAM ì´ˆê¸°í™”
        
        self.llm = ChatOllama(model=settings.LLM_MODEL, temperature=0)
        self.web_search_tool = DuckDuckGoSearchRun()

    async def generate_response(
        self, 
        message: str, 
        kb_id: str, 
        user_id: int, 
        use_web_search: bool = False,
        active_mcp_ids: Optional[List[str]] = None
    ) -> AsyncGenerator[str, None]:
        
        try:
            # [Router] ì§ˆë¬¸ ì˜ë„ ë¶„ì„
            yield json.dumps({"type": "thinking", "thinking": "ğŸ¤” ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."}) + "\n"
            
            router_prompt = ChatPromptTemplate.from_template("""
            Analyze the user's question and choose the best processing mode.
            
            Question: {question}
            
            Options:
            - 'process': Use this if the user wants to execute a logistics/business process (e.g., "dispatch orders", "create routes", "check closed orders").
            - 'search': Use this if the user asks for real-time external info (e.g., weather, news).
            - 'rag': Use this for questions about documents/manuals.
            - 'chat': Use this for general conversation.
            
            Answer (process/search/rag/chat):
            """)
            router_chain = router_prompt | self.llm | StrOutputParser()
            
            # xLAM ëª¨ë“œ ê°•ì œ ì¡°ê±´ (active_mcp_idsì— 'xlam'ì´ ìˆê±°ë‚˜, web_searchê°€ êº¼ì ¸ìˆì„ ë•Œ íŒë‹¨)
            route = "rag"
            if use_web_search:
                route_result = await router_chain.ainvoke({"question": message})
                route = route_result.strip().lower()
            elif "ë°°ì°¨" in message or "ì£¼ë¬¸" in message or "ë£¨íŠ¸" in message or "ì§€ì‹œ" in message:
                route = "process" # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê°ì§€
            
            # --- [MODE 1] xLAM Process Execution ---
            if "process" in route:
                yield json.dumps({"type": "thinking", "thinking": "ğŸš€ xLAM ììœ¨ ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤."}) + "\n"
                async for chunk in self.xlam_service.run_pipeline(message, kb_id, user_id):
                    yield chunk
                return

            # --- [MODE 2] Web Search ---
            if "search" in route:
                yield json.dumps({"type": "thinking", "thinking": "ğŸŒ ì›¹ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤..."}) + "\n"
                try:
                    res = self.web_search_tool.invoke(message)
                    context_text = f"[Web Search Result]\n{res}"
                except:
                    context_text = "ê²€ìƒ‰ ì‹¤íŒ¨"
                    
            # --- [MODE 3] RAG (Document Search) ---
            else:
                yield json.dumps({"type": "thinking", "thinking": f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."}) + "\n"
                retriever = self.vector_service.get_retriever(kb_id, user_id)
                docs = await retriever.ainvoke(message)
                if docs:
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                    yield json.dumps({"type": "thinking", "thinking": f"âœ… ë¬¸ì„œ {len(docs)}ê°œ ì°¸ì¡°"}) + "\n"
                else:
                    context_text = ""
                    yield json.dumps({"type": "thinking", "thinking": "âŒ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ"}) + "\n"

            # ë‹µë³€ ìƒì„± (RAG/General)
            prompt = ChatPromptTemplate.from_template("""
            [ë¬¸ë§¥]
            {context}
            
            [ì§ˆë¬¸]
            {question}
            
            ë‹µë³€í•´ì£¼ì„¸ìš”:
            """)
            chain = prompt | self.llm
            async for chunk in chain.astream({"context": context_text, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                yield json.dumps({"type": "content", "content": content}) + "\n"

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            yield json.dumps({"type": "content", "content": f"Error: {str(e)}"}) + "\n"