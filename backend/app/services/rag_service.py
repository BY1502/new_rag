import json
import asyncio
import os
import torch
from typing import AsyncGenerator, List
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_tool_calling_agent
from app.core.config import settings
from app.services.vector_store import VectorStoreService # ì¶”ê°€

class RAGService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService() # VectorService í™œìš©
        
        self.llm = ChatOllama(
            model=settings.LLM_MODEL,
            temperature=0.1,
        )
        self.qdrant_client = QdrantClient(url=settings.QDRANT_URL)

    async def generate_response(
        self, 
        message: str, 
        kb_id: str, 
        user_id: int, 
        use_web_search: bool = False
    ) -> AsyncGenerator[str, None]:
        
        try:
            # [Redis] ëŒ€í™” ê¸°ë¡ í‚¤ ìƒì„± ì‹œ user_id í¬í•¨ (ì´ë¯¸ APIë‹¨ì—ì„œ ì„¸ì…˜ ê´€ë¦¬í•˜ì§€ë§Œ, ë‚´ë¶€ì ìœ¼ë¡œë„ ë¶„ë¦¬)
            session_id = f"user_{user_id}_default" 

            context_text = ""
            
            # [Vector Search] ìœ ì € IDë¡œ í•„í„°ë§ëœ Retriever ê°€ì ¸ì˜¤ê¸°
            yield json.dumps({"type": "thinking", "thinking": f"ğŸ”’ ìœ ì €({user_id}) ì „ìš© ë°ì´í„° ê²€ìƒ‰ ì¤‘..."}) + "\n"
            
            retriever = self.vector_service.get_retriever(kb_id, user_id)
            docs = await retriever.ainvoke(message)
            
            if docs:
                context_text = "\n\n".join([doc.page_content for doc in docs])
                sources = list(set([doc.metadata.get("source", "Unknown") for doc in docs]))
                yield json.dumps({"type": "thinking", "thinking": f"âœ… ë¬¸ì„œ ë°œê²¬: {', '.join(sources)}"}) + "\n"
            else:
                yield json.dumps({"type": "thinking", "thinking": "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë³¸ì¸ ë¬¸ì„œë§Œ ê²€ìƒ‰ë¨)"}) + "\n"
            
            # --- Chat Prompt ---
            prompt = ChatPromptTemplate.from_template("""
            [ë¬¸ë§¥]
            {context}
            
            [ì§ˆë¬¸]
            {question}
            
            ìœ„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ë§¥ì´ ì—†ìœ¼ë©´ ì•„ëŠ” ëŒ€ë¡œ ë‹µí•˜ì„¸ìš”.
            """)
            
            chain = prompt | self.llm
            
            async for chunk in chain.astream({"context": context_text, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                yield json.dumps({"type": "content", "content": content}) + "\n"

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            yield json.dumps({"type": "content", "content": f"Error: {str(e)}"}) + "\n"