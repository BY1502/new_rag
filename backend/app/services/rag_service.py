import json
import asyncio
import os
from typing import AsyncGenerator, List, Optional
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools import DuckDuckGoSearchRun
from app.core.config import settings
from app.services.vector_store import VectorStoreService
from app.services.xlam_service import XLAMService

class RAGService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService()
        self.xlam_service = XLAMService()
        self.web_search_tool = DuckDuckGoSearchRun()
        # ê¸°ë³¸ LLMì€ ì„¤ì •ê°’ ë”°ë¦„ (fallbackìš©)
        self.default_model = settings.LLM_MODEL

    async def generate_response(
        self, 
        message: str, 
        kb_id: str, 
        user_id: int, 
        model: Optional[str] = None, # âœ… ë™ì  ëª¨ë¸ ë°›ê¸°
        use_web_search: bool = False,
        use_deep_think: bool = False, # âœ… ë”¥ ì”½í‚¹ í”Œë˜ê·¸
        active_mcp_ids: Optional[List[str]] = None
    ) -> AsyncGenerator[str, None]:
        
        try:
            # 1. ëª¨ë¸ ê²°ì • (í”„ë¡ íŠ¸ ìš”ì²­ > í™˜ê²½ë³€ìˆ˜)
            target_model = model if model else self.default_model
            
            # ë§¤ ìš”ì²­ë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ì´ˆê¸°í™” (ë‹¤ì´ë‚˜ë¯¹ ëª¨ë¸ ìŠ¤ìœ„ì¹­ì„ ìœ„í•´)
            # (LangChain ChatOllamaëŠ” ê°€ë²¼ì›Œì„œ ì˜¤ë²„í—¤ë“œê°€ ì ìŒ)
            llm = ChatOllama(model=target_model, temperature=0)

            # --- [Router] ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ---
            route = "rag"
            
            # ë”¥ ì”½í‚¹ì´ ì¼œì ¸ìˆìœ¼ë©´ ë¶„ì„ ê³¼ì •ì„ ë³´ì—¬ì¤Œ
            if use_deep_think:
                yield json.dumps({"type": "thinking", "thinking": f"ğŸ§  Deep Thinking: '{target_model}' ëª¨ë¸ë¡œ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„ ì¤‘..."}) + "\n"
                
                router_prompt = ChatPromptTemplate.from_template("""
                Analyze the user's question and choose the best processing mode.
                Question: {question}
                
                Modes:
                - 'process': Logistics/Business execution (dispatch, order, route).
                - 'search': Real-time info (weather, news).
                - 'rag': Document/Manual based Q&A.
                
                Return ONLY the mode name (process/search/rag).
                """)
                router_chain = router_prompt | llm | StrOutputParser()
                try:
                    route_result = await router_chain.ainvoke({"question": message})
                    route = route_result.strip().lower()
                    yield json.dumps({"type": "thinking", "thinking": f"ğŸ§­ ë¶„ì„ ê²°ê³¼: '{route}' ëª¨ë“œë¡œ ì „ëµ ìˆ˜ë¦½"}) + "\n"
                except:
                    yield json.dumps({"type": "thinking", "thinking": f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨. ê¸°ë³¸ RAG ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤."}) + "\n"
            
            else:
                # ë”¥ ì”½í‚¹ êº¼ì ¸ìˆìœ¼ë©´ í‚¤ì›Œë“œë¡œ ë¹ ë¥´ê²Œ íŒë‹¨
                if use_web_search: route = "search"
                elif any(k in message for k in ["ë°°ì°¨", "ì£¼ë¬¸", "ë£¨íŠ¸", "ì§€ì‹œ"]): route = "process"
                else: route = "rag"

            # --- [MODE 1] xLAM Process ---
            if "process" in route:
                yield json.dumps({"type": "thinking", "thinking": "ğŸš€ xLAM ììœ¨ ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤."}) + "\n"
                # xLAMì—ê²Œë„ ëª¨ë¸ ì •ë³´ ì „ë‹¬í•˜ê³  ì‹¶ìœ¼ë©´ XLAMService ìˆ˜ì • í•„ìš” (ì—¬ê¸°ì„  ìƒëµ)
                async for chunk in self.xlam_service.run_pipeline(message, kb_id, user_id):
                    yield chunk
                return

            context_text = ""
            
            # --- [MODE 2] Web Search ---
            if "search" in route:
                if use_deep_think: yield json.dumps({"type": "thinking", "thinking": "ğŸŒ ìµœì‹  ì •ë³´ë¥¼ ìœ„í•´ ì›¹ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤..."}) + "\n"
                try:
                    res = self.web_search_tool.invoke(message)
                    context_text = f"[Web Search Result]\n{res}"
                except:
                    context_text = "ê²€ìƒ‰ ì‹¤íŒ¨"
                    
            # --- [MODE 3] RAG ---
            else:
                if use_deep_think: yield json.dumps({"type": "thinking", "thinking": f"ğŸ” ì§€ì‹ ë² ì´ìŠ¤({kb_id})ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ íƒìƒ‰ ì¤‘..."}) + "\n"
                retriever = self.vector_service.get_retriever(kb_id, user_id)
                docs = await retriever.ainvoke(message)
                if docs:
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                    if use_deep_think: yield json.dumps({"type": "thinking", "thinking": f"âœ… ë¬¸ì„œ {len(docs)}ê°œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ êµ¬ì„±"}) + "\n"
                else:
                    context_text = ""
                    if use_deep_think: yield json.dumps({"type": "thinking", "thinking": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}) + "\n"

            # ë‹µë³€ ìƒì„±
            prompt = ChatPromptTemplate.from_template("""
            [ë¬¸ë§¥]
            {context}
            [ì§ˆë¬¸]
            {question}
            
            ë‹µë³€í•´ì£¼ì„¸ìš”:
            """)
            chain = prompt | llm
            full_response = ""
            async for chunk in chain.astream({"context": context_text, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += content
                yield json.dumps({"type": "content", "content": content}) + "\n"

            # [Self-Correction] ìê¸° ê²€ì¦ (Deep Thinking ONì¼ ë•Œë§Œ)
            if use_deep_think and len(full_response) > 50:
                yield json.dumps({"type": "thinking", "thinking": "ğŸ›¡ï¸ ë‹µë³€ì˜ ì •í™•ì„±ì„ ìì²´ ê²€ì¦(Self-Reflection) ì¤‘..."}) + "\n"
                reflection_prompt = ChatPromptTemplate.from_template("""
                Question: {question}
                Answer: {answer}
                Rate the answer's accuracy (0-100). Output only the number.
                """)
                try:
                    score = await (reflection_prompt | llm | StrOutputParser()).ainvoke({"question": message, "answer": full_response})
                    score_num = int(''.join(filter(str.isdigit, score)))
                    if score_num > 80:
                         yield json.dumps({"type": "thinking", "thinking": f"âœ¨ ê²€ì¦ ì™„ë£Œ: ì‹ ë¢°ë„ ë†’ìŒ ({score_num}ì )"}) + "\n"
                except: pass

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            yield json.dumps({"type": "content", "content": f"Error: {str(e)}"}) + "\n"