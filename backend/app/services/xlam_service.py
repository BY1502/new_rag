import json
import asyncio
import os
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_tool_calling_agent, AgentExecutor
from app.core.config import settings
from app.services.vector_store import VectorStoreService
from app.services.graph_store import GraphStoreService
from app.tools.logistics import get_logistics_tools

class XLAMService:
    def __init__(self):
        os.environ["OLLAMA_HOST"] = settings.OLLAMA_BASE_URL
        self.vector_service = VectorStoreService()
        self.graph_service = GraphStoreService()
        
        # xLAMìš© ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ (Tool Calling ì§€ì› ëª¨ë¸)
        self.llm = ChatOllama(
            model=settings.LLM_MODEL, 
            temperature=0
        )
        self.tools = get_logistics_tools()

    async def retrieve_manual(self, query: str, kb_id: str, user_id: int):
        """Vector DBì—ì„œ ë§¤ë‰´ì–¼ ê²€ìƒ‰"""
        retriever = self.vector_service.get_retriever(kb_id, user_id)
        docs = await retriever.ainvoke(f"{query} manual process procedure")
        if not docs:
            # ë§¤ë‰´ì–¼ì´ ì•„ì§ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ ë§¤ë‰´ì–¼ (Fallback)
            return """
            [Logistics Process Manual]
            1. Use 'query_closed_orders' to get orders from RDB.
            2. Use 'convert_address_to_coordinates' to add lat/lng.
            3. Use 'run_dispatch_algorithm' to group orders.
            4. Use 'generate_vehicle_routes' to plan routes.
            5. Use 'generate_delivery_instructions' to send to drivers.
            """
        return "\n".join([d.page_content for d in docs])

    async def run_pipeline(self, user_query: str, kb_id: str, user_id: int):
        """xLAM ì‹¤í–‰ íŒŒì´í”„ë¼ì¸"""
        
        yield json.dumps({"type": "thinking", "thinking": "ğŸ“– xLAM: ê´€ë ¨ ë§¤ë‰´ì–¼(Vector DB)ì„ ì°¸ì¡° ì¤‘..."}) + "\n"
        
        # 1. ë§¤ë‰´ì–¼ ê²€ìƒ‰
        manual_context = await self.retrieve_manual(user_query, kb_id, user_id)
        
        # 2. ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""You are xLAM (Large Action Model), an autonomous logistics manager.
            You must execute the logistics process strictly according to the following MANUAL.
            
            [MANUAL]
            {manual_context}
            
            Your goal is to complete the user's request by calling the appropriate tools in the correct order.
            Always check the output of the previous tool before calling the next one.
            """),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])

        # 3. ì—ì´ì „íŠ¸ ìƒì„±
        agent = create_tool_calling_agent(self.llm, self.tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)

        yield json.dumps({"type": "thinking", "thinking": "âš™ï¸ xLAM: í”„ë¡œì„¸ìŠ¤ ê³„íš ìˆ˜ë¦½ ë° ì‹¤í–‰ ì‹œì‘..."}) + "\n"

        # 4. ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë¡
        # (ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ ë‚´ë¶€ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ê±°ë‚˜, ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ë¦¬í„´ë°›ì•„ì•¼ í•¨)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ invoke ê²°ê³¼ë¥¼ ìš”ì•½í•´ì„œ ì „ë‹¬
        
        try:
            # Neo4j ë¡œê·¸: ì‹œì‘
            self.graph_service.log_process_execution(f"sess_{user_id}", "xLAM_Start", "STARTED", user_query)
            
            # ì‹¤í–‰
            result = await agent_executor.ainvoke({"input": user_query})
            
            # Neo4j ë¡œê·¸: ì¢…ë£Œ
            self.graph_service.log_process_execution(f"sess_{user_id}", "xLAM_End", "COMPLETED", result['output'])
            
            yield json.dumps({"type": "thinking", "thinking": "âœ… ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}) + "\n"
            yield json.dumps({"type": "content", "content": result['output']}) + "\n"
            
        except Exception as e:
            self.graph_service.log_process_execution(f"sess_{user_id}", "xLAM_Error", "FAILED", str(e))
            yield json.dumps({"type": "content", "content": f"Error executing xLAM pipeline: {str(e)}"}) + "\n"